{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimation import estimate_text_distribution\n",
    "from src.MLE import MLE\n",
    "import pandas as pd\n",
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OJS Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ojs_ed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 monolingual English Education research article abstracts (n = 27,010)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data_21: 212521 sentences\n",
      "ai_data_21: 451240 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "print(f\"human_data_21: {human_data_21.shape[0]} sentences\")\n",
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "print(f\"ai_data_21: {ai_data_21.shape[0]} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.059,     0.001,     0.059\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.086,     0.001,     0.061\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.109,     0.002,     0.059\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.133,     0.002,     0.058\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.155,     0.002,     0.055\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.177,     0.002,     0.052\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.199,     0.002,     0.049\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.220,     0.002,     0.045\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.242,     0.002,     0.042\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.263,     0.002,     0.038\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.284,     0.003,     0.034\n",
      "+---------------------------------+\n",
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.053,     0.002,     0.053\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.063,     0.002,     0.038\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.075,     0.002,     0.025\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.085,     0.002,     0.010\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.097,     0.002,     0.003\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.105,     0.002,     0.020\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.116,     0.002,     0.034\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.126,     0.002,     0.049\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.135,     0.003,     0.065\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.146,     0.003,     0.079\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.156,     0.003,     0.094\n",
      "+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_21.parquet\",f\"data/training_data/{name}/ai_data_21.parquet\",f\"distribution/{name}_21.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_21.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 monolingual English Education research article abstracts + possibly google translated abstracts (n = 48,391)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data: 354945 sentences\n",
      "ai_data: 697274 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "human_data_21_translated = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_21.parquet\")\n",
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "ai_data_21_translated = pd.read_parquet(f\"data/training_data/{name}/ai_data_translated_21.parquet\")\n",
    "# stack the data\n",
    "human_data = pd.concat([human_data_21, human_data_21_translated])\n",
    "print(f\"human_data: {human_data.shape[0]} sentences\")\n",
    "ai_data = pd.concat([ai_data_21, ai_data_21_translated])\n",
    "print(f\"ai_data: {ai_data.shape[0]} sentences\")\n",
    "# save the data to new parquet files\n",
    "human_data.to_parquet(f\"data/training_data/{name}/human_data_plus_translated_21.parquet\")\n",
    "ai_data.to_parquet(f\"data/training_data/{name}/ai_data_plus_translated_21.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.080,     0.002,     0.080\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.110,     0.002,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.138,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.164,     0.002,     0.089\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.188,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.213,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.238,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.260,     0.002,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.285,     0.002,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.308,     0.003,     0.083\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.331,     0.003,     0.081\n",
      "+---------------------------------+\n",
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.051,     0.002,     0.051\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.065,     0.002,     0.040\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.079,     0.002,     0.029\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.092,     0.002,     0.017\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.106,     0.002,     0.006\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.118,     0.003,     0.007\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.130,     0.003,     0.020\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.143,     0.003,     0.032\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.154,     0.003,     0.046\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.167,     0.003,     0.058\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.180,     0.003,     0.070\n",
      "+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_plus_translated_21.parquet\",f\"data/training_data/{name}/ai_data_plus_translated_21.parquet\",f\"distribution/{name}_plus_translated_21.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_plus_translated_21.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 + 2020 monolingual English Education research article abstracts (n = 46,128)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data: 365359 sentences\n",
      "ai_data: 771580 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "human_data_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_20.parquet\")\n",
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "ai_data_20 = pd.read_parquet(f\"data/training_data/{name}/ai_data_20.parquet\")\n",
    "# stack the data\n",
    "human_data = pd.concat([human_data_21,human_data_20])\n",
    "print(f\"human_data: {human_data.shape[0]} sentences\")\n",
    "ai_data = pd.concat([ai_data_21,ai_data_20])\n",
    "print(f\"ai_data: {ai_data.shape[0]} sentences\")\n",
    "# save the data to new parquet files\n",
    "human_data.to_parquet(f\"data/training_data/{name}/human_data_21_20.parquet\")\n",
    "ai_data.to_parquet(f\"data/training_data/{name}/ai_data_21_20.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.059,     0.001,     0.059\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.085,     0.001,     0.060\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.109,     0.002,     0.059\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.133,     0.002,     0.058\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.155,     0.002,     0.055\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.178,     0.002,     0.053\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.200,     0.002,     0.050\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.221,     0.002,     0.046\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.244,     0.002,     0.044\n"
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_21_20.parquet\",f\"data/training_data/{name}/ai_data_21_20.parquet\",f\"distribution/{name}_21_20.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_21_20.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print( \"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 + 2020 monolingual English Education research article abstracts + translated abstracts (n = 83,992)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "human_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_21.parquet\")\n",
    "human_data_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_20.parquet\")\n",
    "human_data_translated_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_20.parquet\")\n",
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "ai_data_translated_21 = pd.read_parquet(f\"data/training_data/ai_data_translated_21.parquet\")\n",
    "ai_data_20 = pd.read_parquet(f\"data/training_data/{name}/ai_data_20.parquet\")\n",
    "# stack the data\n",
    "human_data = pd.concat([human_data_21, human_data_translated_21, human_data_20, human_data_translated_20])\n",
    "print(f\"human_data: {human_data.shape[0]} sentences\")\n",
    "ai_data = pd.concat([ai_data_21, ai_data_translated_21, ai_data_20])\n",
    "print(f\"ai_data: {ai_data.shape[0]} sentences\")\n",
    "# save the data to new parquet files\n",
    "human_data.to_parquet(f\"data/training_data/{name}/human_data_translated_21_20.parquet\")\n",
    "ai_data.to_parquet(f\"data/training_data/{name}/ai_data_translated_21_20.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_translated_21_20.parquet\",f\"data/training_data/{name}/ai_data_translated_21_20.parquet\",f\"distribution/{name}_translated_21_20.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_translated_21_20.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print( \"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 possibly translated abstracts (n = 21,381)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_21.parquet\")\n",
    "print(f\"human_data_translated_21: {human_data_translated_21.shape[0]} sentences\")\n",
    "ai_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_translated_21.parquet\")\n",
    "print(f\"ai_data_translated_21: {ai_data_translated_21.shape[0]} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_translated_21.parquet\",f\"data/training_data/{name}/ai_data_translated_21.parquet\",f\"distribution/{name}_translated_21.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_translated_21.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print( \"+---------------------------------+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
