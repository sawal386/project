{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.estimation import estimate_text_distribution\n",
    "from src.MLE import MLE\n",
    "import pandas as pd\n",
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OJS Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"ojs_ed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 monolingual English Education research article abstracts (n = 27,010)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data_21: 212521 sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identifier</th>\n",
       "      <th>date</th>\n",
       "      <th>creator</th>\n",
       "      <th>publisher</th>\n",
       "      <th>title</th>\n",
       "      <th>human_sentence</th>\n",
       "      <th>source</th>\n",
       "      <th>relation</th>\n",
       "      <th>language</th>\n",
       "      <th>rights</th>\n",
       "      <th>predicted_language</th>\n",
       "      <th>predicted_fos</th>\n",
       "      <th>description</th>\n",
       "      <th>sent_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[2021-07-24]</td>\n",
       "      <td>[-, Eni Dwi Kurniawati]</td>\n",
       "      <td>[Pusat Studi Pendidikan Kreatifitas Anak Wadas...</td>\n",
       "      <td>[Peningkatan Kemampuan Guru Membuat Adminstras...</td>\n",
       "      <td>[this, research, departs, from, the, lack, of,...</td>\n",
       "      <td>[Educreative : Jurnal Pendidikan Kreativitas A...</td>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Copyright (c) 2021 Educreative : Jurnal Pendi...</td>\n",
       "      <td>en</td>\n",
       "      <td>Education</td>\n",
       "      <td>[This research departs from the lack of teache...</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[2021-07-24]</td>\n",
       "      <td>[-, Eni Dwi Kurniawati]</td>\n",
       "      <td>[Pusat Studi Pendidikan Kreatifitas Anak Wadas...</td>\n",
       "      <td>[Peningkatan Kemampuan Guru Membuat Adminstras...</td>\n",
       "      <td>[it, was, discovered, when, carrying, out, coa...</td>\n",
       "      <td>[Educreative : Jurnal Pendidikan Kreativitas A...</td>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Copyright (c) 2021 Educreative : Jurnal Pendi...</td>\n",
       "      <td>en</td>\n",
       "      <td>Education</td>\n",
       "      <td>[This research departs from the lack of teache...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[2021-07-24]</td>\n",
       "      <td>[-, Eni Dwi Kurniawati]</td>\n",
       "      <td>[Pusat Studi Pendidikan Kreatifitas Anak Wadas...</td>\n",
       "      <td>[Peningkatan Kemampuan Guru Membuat Adminstras...</td>\n",
       "      <td>[there, are, still, many, teachers, who, do, n...</td>\n",
       "      <td>[Educreative : Jurnal Pendidikan Kreativitas A...</td>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Copyright (c) 2021 Educreative : Jurnal Pendi...</td>\n",
       "      <td>en</td>\n",
       "      <td>Education</td>\n",
       "      <td>[This research departs from the lack of teache...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[2021-07-24]</td>\n",
       "      <td>[-, Eni Dwi Kurniawati]</td>\n",
       "      <td>[Pusat Studi Pendidikan Kreatifitas Anak Wadas...</td>\n",
       "      <td>[Peningkatan Kemampuan Guru Membuat Adminstras...</td>\n",
       "      <td>[therefore, this, study, aims, to, improve, th...</td>\n",
       "      <td>[Educreative : Jurnal Pendidikan Kreativitas A...</td>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Copyright (c) 2021 Educreative : Jurnal Pendi...</td>\n",
       "      <td>en</td>\n",
       "      <td>Education</td>\n",
       "      <td>[This research departs from the lack of teache...</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[2021-07-24]</td>\n",
       "      <td>[-, Eni Dwi Kurniawati]</td>\n",
       "      <td>[Pusat Studi Pendidikan Kreatifitas Anak Wadas...</td>\n",
       "      <td>[Peningkatan Kemampuan Guru Membuat Adminstras...</td>\n",
       "      <td>[the, research, method, used, is, school, acti...</td>\n",
       "      <td>[Educreative : Jurnal Pendidikan Kreativitas A...</td>\n",
       "      <td>[https://educreative.id/index.php/edu/article/...</td>\n",
       "      <td>[eng]</td>\n",
       "      <td>[Copyright (c) 2021 Educreative : Jurnal Pendi...</td>\n",
       "      <td>en</td>\n",
       "      <td>Education</td>\n",
       "      <td>[This research departs from the lack of teache...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          identifier          date  \\\n",
       "0  [https://educreative.id/index.php/edu/article/...  [2021-07-24]   \n",
       "1  [https://educreative.id/index.php/edu/article/...  [2021-07-24]   \n",
       "2  [https://educreative.id/index.php/edu/article/...  [2021-07-24]   \n",
       "3  [https://educreative.id/index.php/edu/article/...  [2021-07-24]   \n",
       "4  [https://educreative.id/index.php/edu/article/...  [2021-07-24]   \n",
       "\n",
       "                   creator                                          publisher  \\\n",
       "0  [-, Eni Dwi Kurniawati]  [Pusat Studi Pendidikan Kreatifitas Anak Wadas...   \n",
       "1  [-, Eni Dwi Kurniawati]  [Pusat Studi Pendidikan Kreatifitas Anak Wadas...   \n",
       "2  [-, Eni Dwi Kurniawati]  [Pusat Studi Pendidikan Kreatifitas Anak Wadas...   \n",
       "3  [-, Eni Dwi Kurniawati]  [Pusat Studi Pendidikan Kreatifitas Anak Wadas...   \n",
       "4  [-, Eni Dwi Kurniawati]  [Pusat Studi Pendidikan Kreatifitas Anak Wadas...   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Peningkatan Kemampuan Guru Membuat Adminstras...   \n",
       "1  [Peningkatan Kemampuan Guru Membuat Adminstras...   \n",
       "2  [Peningkatan Kemampuan Guru Membuat Adminstras...   \n",
       "3  [Peningkatan Kemampuan Guru Membuat Adminstras...   \n",
       "4  [Peningkatan Kemampuan Guru Membuat Adminstras...   \n",
       "\n",
       "                                      human_sentence  \\\n",
       "0  [this, research, departs, from, the, lack, of,...   \n",
       "1  [it, was, discovered, when, carrying, out, coa...   \n",
       "2  [there, are, still, many, teachers, who, do, n...   \n",
       "3  [therefore, this, study, aims, to, improve, th...   \n",
       "4  [the, research, method, used, is, school, acti...   \n",
       "\n",
       "                                              source  \\\n",
       "0  [Educreative : Jurnal Pendidikan Kreativitas A...   \n",
       "1  [Educreative : Jurnal Pendidikan Kreativitas A...   \n",
       "2  [Educreative : Jurnal Pendidikan Kreativitas A...   \n",
       "3  [Educreative : Jurnal Pendidikan Kreativitas A...   \n",
       "4  [Educreative : Jurnal Pendidikan Kreativitas A...   \n",
       "\n",
       "                                            relation language  \\\n",
       "0  [https://educreative.id/index.php/edu/article/...    [eng]   \n",
       "1  [https://educreative.id/index.php/edu/article/...    [eng]   \n",
       "2  [https://educreative.id/index.php/edu/article/...    [eng]   \n",
       "3  [https://educreative.id/index.php/edu/article/...    [eng]   \n",
       "4  [https://educreative.id/index.php/edu/article/...    [eng]   \n",
       "\n",
       "                                              rights predicted_language  \\\n",
       "0  [Copyright (c) 2021 Educreative : Jurnal Pendi...                 en   \n",
       "1  [Copyright (c) 2021 Educreative : Jurnal Pendi...                 en   \n",
       "2  [Copyright (c) 2021 Educreative : Jurnal Pendi...                 en   \n",
       "3  [Copyright (c) 2021 Educreative : Jurnal Pendi...                 en   \n",
       "4  [Copyright (c) 2021 Educreative : Jurnal Pendi...                 en   \n",
       "\n",
       "  predicted_fos                                        description  \\\n",
       "0     Education  [This research departs from the lack of teache...   \n",
       "1     Education  [This research departs from the lack of teache...   \n",
       "2     Education  [This research departs from the lack of teache...   \n",
       "3     Education  [This research departs from the lack of teache...   \n",
       "4     Education  [This research departs from the lack of teache...   \n",
       "\n",
       "   sent_length  \n",
       "0           22  \n",
       "1           13  \n",
       "2           16  \n",
       "3           30  \n",
       "4            9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "print(f\"human_data_21: {human_data_21.shape[0]} sentences\")\n",
    "human_data_21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ai_data_21: 451240 sentences\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[the, goal, of, this, article, is, to, delve, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[additionally, it, aims, to, examine, the, ben...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[to, achieve, these, objectives, the, study, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[it, acknowledges, that, the, way, students, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[by, understanding, and, implementing, effecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         ai_sentence\n",
       "0  [the, goal, of, this, article, is, to, delve, ...\n",
       "1  [additionally, it, aims, to, examine, the, ben...\n",
       "2  [to, achieve, these, objectives, the, study, b...\n",
       "3  [it, acknowledges, that, the, way, students, a...\n",
       "4  [by, understanding, and, implementing, effecti..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "print(f\"ai_data_21: {ai_data_21.shape[0]} sentences\")\n",
    "ai_data_21.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.060,     0.001,     0.060\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.086,     0.002,     0.061\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.110,     0.002,     0.060\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.133,     0.002,     0.058\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.156,     0.002,     0.056\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.178,     0.002,     0.053\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.200,     0.002,     0.050\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.222,     0.002,     0.047\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.243,     0.002,     0.043\n"
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_21.parquet\",f\"data/training_data/{name}/ai_data_21.parquet\",f\"distribution/{name}_21.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_21.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 monolingual English Education research article abstracts + possibly google translated abstracts (n = 48,391)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "human_data_21_translated = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_21.parquet\")\n",
    "human_data = pd.concat([human_data_21, human_data_21_translated])\n",
    "print(f\"human_data: {human_data.shape[0]} sentences\")\n",
    "# save the data to combined parquet file\n",
    "human_data.to_parquet(f\"data/training_data/{name}/human_data_plus_translated_21.parquet\")\n",
    "# show the first few rows\n",
    "human_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data: 354945 sentences\n",
      "ai_data: 697274 sentences\n"
     ]
    }
   ],
   "source": [
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "ai_data_21_translated = pd.read_parquet(f\"data/training_data/{name}/ai_data_translated_21.parquet\")\n",
    "ai_data = pd.concat([ai_data_21, ai_data_21_translated])\n",
    "print(f\"ai_data: {ai_data.shape[0]} sentences\")\n",
    "# save the data to new parquet files\n",
    "ai_data.to_parquet(f\"data/training_data/{name}/ai_data_plus_translated_21.parquet\")\n",
    "# show the first few rows\n",
    "ai_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.080,     0.002,     0.080\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.110,     0.002,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.138,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.164,     0.002,     0.089\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.188,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.213,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.238,     0.002,     0.088\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.260,     0.002,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.285,     0.002,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.308,     0.003,     0.083\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.331,     0.003,     0.081\n",
      "+---------------------------------+\n",
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.051,     0.002,     0.051\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.065,     0.002,     0.040\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.079,     0.002,     0.029\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.092,     0.002,     0.017\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.106,     0.002,     0.006\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.118,     0.003,     0.007\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.130,     0.003,     0.020\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.143,     0.003,     0.032\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.154,     0.003,     0.046\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.167,     0.003,     0.058\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.180,     0.003,     0.070\n",
      "+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_plus_translated_21.parquet\",f\"data/training_data/{name}/ai_data_plus_translated_21.parquet\",f\"distribution/{name}_plus_translated_21.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_plus_translated_21.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 + 2020 monolingual English Education research article abstracts (n = 46,128)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data: 365359 sentences\n",
      "ai_data: 771580 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "human_data_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_20.parquet\")\n",
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "ai_data_20 = pd.read_parquet(f\"data/training_data/{name}/ai_data_20.parquet\")\n",
    "# stack the data\n",
    "human_data = pd.concat([human_data_21,human_data_20])\n",
    "print(f\"human_data: {human_data.shape[0]} sentences\")\n",
    "ai_data = pd.concat([ai_data_21,ai_data_20])\n",
    "print(f\"ai_data: {ai_data.shape[0]} sentences\")\n",
    "# save the data to new parquet files\n",
    "human_data.to_parquet(f\"data/training_data/{name}/human_data_21_20.parquet\")\n",
    "ai_data.to_parquet(f\"data/training_data/{name}/ai_data_21_20.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.059,     0.001,     0.059\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.085,     0.001,     0.060\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.109,     0.002,     0.059\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.133,     0.002,     0.058\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.155,     0.002,     0.055\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.178,     0.002,     0.053\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.200,     0.002,     0.050\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.221,     0.002,     0.046\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.244,     0.002,     0.044\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.265,     0.002,     0.040\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.286,     0.002,     0.036\n",
      "+---------------------------------+\n",
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.049,     0.002,     0.049\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.060,     0.002,     0.035\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.071,     0.002,     0.021\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.082,     0.002,     0.007\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.094,     0.002,     0.006\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.103,     0.002,     0.022\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.113,     0.002,     0.037\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.124,     0.002,     0.051\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.132,     0.002,     0.068\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.225,     0.143,     0.002,     0.082\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.250,     0.153,     0.003,     0.097\n",
      "+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_21_20.parquet\",f\"data/training_data/{name}/ai_data_21_20.parquet\",f\"distribution/{name}_21_20.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_21_20.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")\n",
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print( \"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 + 2020 monolingual English Education research article abstracts + translated abstracts (n = 83,992)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data: 617675 sentences\n",
      "ai_data: 1017614 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_21.parquet\")\n",
    "human_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_21.parquet\")\n",
    "human_data_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_20.parquet\")\n",
    "human_data_translated_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_20.parquet\")\n",
    "ai_data_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_21.parquet\")\n",
    "ai_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_translated_21.parquet\")\n",
    "ai_data_20 = pd.read_parquet(f\"data/training_data/{name}/ai_data_20.parquet\")\n",
    "# stack the data\n",
    "human_data = pd.concat([human_data_21, human_data_translated_21, human_data_20, human_data_translated_20])\n",
    "print(f\"human_data: {human_data.shape[0]} sentences\")\n",
    "ai_data = pd.concat([ai_data_21, ai_data_translated_21, ai_data_20])\n",
    "print(f\"ai_data: {ai_data.shape[0]} sentences\")\n",
    "# save the data to new parquet files\n",
    "human_data.to_parquet(f\"data/training_data/{name}/human_data_translated_21_20.parquet\")\n",
    "ai_data.to_parquet(f\"data/training_data/{name}/ai_data_translated_21_20.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.090,     0.002,     0.090\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.121,     0.002,     0.096\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN EVALUATION SET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:90\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 91\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_q_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:91\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_translated_21_20.parquet\",f\"data/training_data/{name}/ai_data_translated_21_20.parquet\",f\"distribution/{name}_translated_21_20.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_translated_21_20.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.029,     0.002,     0.029\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.041,     0.002,     0.016\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.054,     0.002,     0.004\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.064,     0.002,     0.011\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.078,     0.002,     0.022\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.087,     0.002,     0.038\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.098,     0.002,     0.052\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.109,     0.002,     0.066\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/ojs_ed/translated/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:87\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print( \"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2020 monolingual English Education research article abstracts (n = 19,118)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data_20: 152838 sentences\n",
      "ai_data_20: 320340 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_20 = pd.read_parquet(f\"data/training_data/{name}/human_data_20.parquet\")\n",
    "print(f\"human_data_20: {human_data_20.shape[0]} sentences\")\n",
    "ai_data_20 = pd.read_parquet(f\"data/training_data/{name}/ai_data_20.parquet\")\n",
    "print(f\"ai_data_20: {ai_data_20.shape[0]} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.062,     0.001,     0.062\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.089,     0.002,     0.064\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.113,     0.002,     0.063\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.137,     0.002,     0.062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN EVALUATION SET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:87\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_20.parquet\",f\"data/training_data/{name}/ai_data_20.parquet\",f\"distribution/{name}_20.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_20.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.049,     0.002,     0.049\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.060,     0.002,     0.035\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.071,     0.002,     0.021\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.082,     0.002,     0.007\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.094,     0.002,     0.006\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.103,     0.002,     0.022\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.113,     0.002,     0.037\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.124,     0.002,     0.051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/ojs_ed/translated/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:87\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training data: OJS 2021 possibly translated abstracts (n = 21,381)<br><br>\n",
    "Validation data: OJS Jan - Nov 2022 English Education abstracts<br>\n",
    "    Monolingual: 26,094 abstracts, 196,928 sentences<br>\n",
    "    Translated: 17,385 abstracts, 119,340 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_data_translated_21: 142424 sentences\n",
      "ai_data_translated_21: 246034 sentences\n"
     ]
    }
   ],
   "source": [
    "human_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/human_data_translated_21.parquet\")\n",
    "print(f\"human_data_translated_21: {human_data_translated_21.shape[0]} sentences\")\n",
    "ai_data_translated_21 = pd.read_parquet(f\"data/training_data/{name}/ai_data_translated_21.parquet\")\n",
    "print(f\"ai_data_translated_21: {ai_data_translated_21.shape[0]} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.163,     0.003,     0.163\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.196,     0.003,     0.171\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN EVALUATION SET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 9\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:87\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:88\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mPrecomputes log probabilities for each data point in the dataset, according to both distributions P and Q.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mwhen calculating the total log probability of sentences.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution P, i.e. generated by humans\u001b[39;00m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_p_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# call function estimate_text_distribution to get the AI content distribution & human content distribution\n",
    "estimate_text_distribution(f\"data/training_data/{name}/human_data_translated_21.parquet\",f\"data/training_data/{name}/ai_data_translated_21.parquet\",f\"distribution/{name}_translated_21.parquet\")\n",
    "# load the word occurrences frequency into our framework\n",
    "model=MLE(f\"distribution/{name}_translated_21.parquet\")\n",
    "# validate our method using mixed corpus with known ground truth alpha\n",
    "# this is the guaranteed human-written evaluation set\n",
    "print(f\"HUMAN EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/{name}/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print(\"+---------------------------------+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.000,     0.097,     0.003,     0.097\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.025,     0.119,     0.003,     0.094\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.050,     0.142,     0.003,     0.092\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.075,     0.160,     0.004,     0.085\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.100,     0.182,     0.004,     0.082\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.125,     0.200,     0.004,     0.075\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.150,     0.218,     0.004,     0.068\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.175,     0.239,     0.004,     0.064\n",
      "Ground Truth,Prediction,        CI,     Error\n",
      "     0.200,     0.255,     0.004,     0.055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.025\u001b[39m,\u001b[38;5;241m0.05\u001b[39m,\u001b[38;5;241m0.075\u001b[39m,\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.125\u001b[39m,\u001b[38;5;241m0.15\u001b[39m,\u001b[38;5;241m0.175\u001b[39m,\u001b[38;5;241m0.2\u001b[39m,\u001b[38;5;241m0.225\u001b[39m,\u001b[38;5;241m0.25\u001b[39m]:\n\u001b[0;32m----> 4\u001b[0m     estimated,ci\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/validation_data/ojs_ed/translated/ground_truth_alpha_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43malpha\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mabs\u001b[39m(estimated\u001b[38;5;241m-\u001b[39malpha)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGround Truth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCI\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:163\u001b[0m, in \u001b[0;36mMLE.inference\u001b[0;34m(self, inference_file_path)\u001b[0m\n\u001b[1;32m    161\u001b[0m data \u001b[38;5;241m=\u001b[39m inference_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set))\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Infer the confidence interval for the mixing parameter alpha\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m confidence_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap_alpha_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Calculate and round the mean of the confidence interval and its half-width\u001b[39;00m\n\u001b[1;32m    165\u001b[0m solution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(confidence_interval), \u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:116\u001b[0m, in \u001b[0;36mMLE.bootstrap_alpha_inference\u001b[0;34m(self, data, n_bootstrap)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03mInfers the mixing parameter (alpha) between two distributions (P and Q) using bootstrap resampling.\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;03m    representing a 95% confidence interval for the mixing parameter alpha.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Precomputes log probabilities for each data point in the entire dataset\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m full_log_p_values, full_log_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute_log_probabilities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# To store estimated alpha values from each bootstrap sample\u001b[39;00m\n\u001b[1;32m    118\u001b[0m alpha_values_bootstrap \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:90\u001b[0m, in \u001b[0;36mMLE.precompute_log_probabilities\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_q_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:91\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 91\u001b[0m                 \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_q_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tokens_set\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "File \u001b[0;32m~/Work/Mapping-the-Increasing-Use-of-LLMs-in-Scientific-Papers/src/MLE.py:91\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     87\u001b[0m log_p_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_p_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[1;32m     88\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_one_minus_p_hat[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Calculate the log probabilities for each sample in 'data' under distribution Q, i.e. generated by AI\u001b[39;00m\n\u001b[1;32m     90\u001b[0m log_q_values \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_q_hat\u001b[38;5;241m.\u001b[39mget(t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m13.8\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m---> 91\u001b[0m                 \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_one_minus_q_hat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_tokens_set \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(log_p_values), np\u001b[38;5;241m.\u001b[39marray(log_q_values)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is the possibly google translated evaluation set\n",
    "print(f\"HUMAN + GOOGLE TRANSLATE (?) EVALUATION SET\")\n",
    "for alpha in [0,0.025,0.05,0.075,0.1,0.125,0.15,0.175,0.2,0.225,0.25]:\n",
    "    estimated,ci=model.inference(f\"data/validation_data/ojs_ed/translated/ground_truth_alpha_{alpha}.parquet\")\n",
    "    error=abs(estimated-alpha)\n",
    "    print(f\"{'Ground Truth':>10},{'Prediction':>10},{'CI':>10},{'Error':>10}\")\n",
    "    print(f\"{alpha:10.3f},{estimated:10.3f},{ci:10.3f},{error:10.3f}\")\n",
    "print( \"+---------------------------------+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
